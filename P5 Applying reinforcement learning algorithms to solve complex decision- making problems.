import numpy as np 
grid_size = 5
episodes = 500
alpha = 0.1
gamma = 0.9
epsilon = 0.2
q_table = np.zeros((grid_size, grid_size, 4)) 
goal = (4, 4)
obstacles = [(1, 1), (2, 2), (3, 3)]
actions = [(0, -1), (0, 1), (-1, 0), (1, 0)]
def is_valid(x, y):
    return 0 <= x < grid_size and 0 <= y < grid_size and (x, y) not in obstacles

def choose_action(x, y):
    if np.random.uniform() < epsilon: 
      return np.random.randint(4)
    return np.argmax(q_table[x, y])

for _ in range(episodes): 
    x, y = 0, 0
    while (x, y) != goal:
      action = choose_action(x, y) 
      dx, dy = actions[action] 
      new_x, new_y = x + dx, y + dy

    if not is_valid(new_x, new_y): 
      new_x, new_y = x, y
      reward = -1
    elif (new_x, new_y) == goal: 
      reward = 100
    else:
      reward = -0.1

    q_table[x, y, action] += alpha * (reward + gamma * np.max(q_table[new_x, new_y]) - q_table[x, y, action])
    x, y = new_x, new_y 
    if (x, y) == goal:
      break

x, y = 0, 0
path = [(x, y)] 
while (x, y)!= goal:
    action = np.argmax(q_table[x, y]) 
    dx, dy = actions[action]
    x, y = x + dx, y + dy 
    path.append((x, y))
    if not is_valid(x, y): 
      break
print("Learned path:",path)
