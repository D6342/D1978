# Code A: Using TPOT for Hyperparameter Tuning 
#Install TPOT
#!pip install tpot

from tpot import TPOTClassifier
from sklearn.model_selection import train_test_split 
from sklearn.datasets import load_iris

# Load dataset 
data = load_iris()
X, y = data.data, data.target
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# Initialize TPOT for evolutionary optimization 
tpot = TPOTClassifier(
    generations=5,	# Number of generations to evolve
    population_size=20, # Number of individuals in each generation
    verbosity=2, 
    random_state=42,
    n_jobs=-1
)

# Run optimization 
tpot.fit(X_train, y_train)

# Evaluate the best model
print("Test Accuracy:", tpot.score(X_test, y_test))

# Export the optimized pipeline as Python code 
tpot.export("best_pipeline.py")


#code b: Using Optuna for Bayesian Optimization 
import optuna
from sklearn.datasets import load_iris
from sklearn.ensemble import RandomForestClassifier 
from sklearn.model_selection import cross_val_score

# Suppress Optuna logging 
optuna.logging.set_verbosity(optuna.logging.WARNING)

# Load dataset 
data = load_iris()
X, y = data.data, data.target

# Objective function for optimization 
def objective(trial):
    n_estimators = trial.suggest_int('n_estimators', 10, 200)
    max_depth = trial.suggest_int('max_depth', 1, 32)
    min_samples_split = trial.suggest_int('min_samples_split', 2, 10) 
    model = RandomForestClassifier(
        n_estimators=n_estimators, 
        max_depth=max_depth, 
        min_samples_split=min_samples_split, 
        random_state=42
    )
    scores = cross_val_score(model, X, y, cv=3, scoring='accuracy') 
    return scores.mean()

# Create and run study
study = optuna.create_study(direction='maximize') 
study.optimize(objective, n_trials=50)

# Print only the desired output
print("Best parameters:", study.best_params)
print("Best cross-validation accuracy:", study.best_value)


